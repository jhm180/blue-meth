{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from os import path\n",
      "import glob\n",
      "import re\n",
      "from datetime import datetime,date,timedelta\n",
      "from pandas.io.parsers import read_csv\n",
      "import numpy as np\n",
      "from itertools import repeat\n",
      "import sys\n",
      "import traceback\n",
      "\n",
      "ADD = 1\n",
      "REMOVE = 2\n",
      "READD = 3\n",
      "PRICE_CHANGE = 4\n",
      "\n",
      "DATA_PATH = '/home/oliver/rapnet_data'\n",
      "COL_NAMES = ['LotNum', 'Owner', 'Shape', 'Carat', 'Color', 'Clarity', 'Cut Grade', 'Price', 'PctRap',\n",
      " 'Cert', 'Depth', 'Table', 'Girdle', 'Culet', 'Polish', 'Sym', 'Fluor', 'Meas', 'Comment',\n",
      " 'NumStones', 'CertNum', 'StockNum', 'Make', 'Date', 'City', 'State', 'Country', 'Image']\n",
      "\n",
      "def read_daily_file(file_date):\n",
      "    compression = None\n",
      "    daily_file = path.join(DATA_PATH, 'Rapnet_{0}_Main.csv'.format(file_date))\n",
      "    if not path.exists(daily_file):\n",
      "        daily_file = daily_file + '.gz'\n",
      "        compression = \"gzip\"\n",
      "    if not path.exists(daily_file):\n",
      "        return None\n",
      "    return read_csv(daily_file, compression = compression, names = COL_NAMES, header = 0,\n",
      "                    dtype = {'Owner':str, 'CertNum':str})\n",
      "\n",
      "def filter_data(df):\n",
      "    indices = []\n",
      "    for k, grpdf in df.groupby(['Owner','CertNum']):\n",
      "        if len(grpdf) == 1:\n",
      "            indices.append(grpdf.index[0])\n",
      "    nodupes = df.loc[indices]\n",
      "    if len(df) - len(nodupes) > 0:\n",
      "        print 'Dupe filtering: dropped {0} duplicate of {1} rows'.format(len(df) - len(nodupes), len(df))\n",
      "    stones = nodupes[[c for c in COL_NAMES if not c in {'CertNum','Owner'}]]\n",
      "    stones.index = pd.MultiIndex.from_tuples([(owner, cert) for owner, cert in nodupes[['Owner','CertNum']].values],\n",
      "                                             names = ['Owner', 'CertNum'])\n",
      "    return stones\n",
      "\n",
      "def gen_file_dates(start_day = datetime(2013,3,3)):\n",
      "    oneday = timedelta(1)\n",
      "    today = datetime.today()\n",
      "    cur_day = start_day\n",
      "    while cur_day <= today:\n",
      "        yield cur_day\n",
      "        cur_day = cur_day + oneday\n",
      "        \n",
      "def get_latest(grp):\n",
      "    grp.sortlevel(level='event_day', inplace=True, ascending = False)\n",
      "    return grp.iloc[0]\n",
      "\n",
      "def build_cache():\n",
      "    all_records = []\n",
      "    all_owners = []\n",
      "    prev = []\n",
      "    i = 0\n",
      "    def reindex(idx_day, idx_event, df):\n",
      "        df.index = pd.MultiIndex.from_tuples([(idx_event, owner, cert, idx_day) for owner, cert in df.index],\n",
      "                                             names = ['event_type','Owner','CertNum','event_day'])\n",
      "            \n",
      "    for cur_day in gen_file_dates(): #glob.glob(path.join(DATA_PATH, \"Rapnet_*_Main.csv*\")):\n",
      "        file_date = cur_day.strftime('%Y%m%d')\n",
      "        if i > 2: # remove after debugging\n",
      "            break\n",
      "        i += 1\n",
      "        now = datetime.now()\n",
      "        print 'loading file for {0}...'.format(file_date),\n",
      "        df = read_daily_file(file_date)\n",
      "        active = filter_data(df)\n",
      "        if len(prev):\n",
      "            new_stones = active.loc[active.index - prev.index]\n",
      "            kept_stones = prev.loc[active.index & prev.index]\n",
      "            readds = []\n",
      "            if len(new_stones):           \n",
      "                # cross section of data - we only want things were previously \n",
      "                # removed, to see if any of the new_stones are actually re-adds\n",
      "                all_removals = []\n",
      "                try:\n",
      "                    all_removals = all_records.xs(REMOVE, level = 'event_type')\n",
      "                except KeyError:\n",
      "                    pass\n",
      "                if len(all_removals):\n",
      "                    prev_removals = all_removals.groupby(level=['Owner','CertNum']).apply(get_latest)\n",
      "                    if len(prev_removals):\n",
      "                        readds = new_stones.loc[new_stones.index & prev_removals.index]\n",
      "                        if len(readds):\n",
      "                            newonly = new_stones.index - readds.index\n",
      "                            print newonly\n",
      "                            if len(newonly):\n",
      "                                new_stones = new_stones.loc[newonly]\n",
      "                            else:\n",
      "                                new_stones = []\n",
      "\n",
      "            if len(new_stones):\n",
      "                reindex(cur_day, ADD, new_stones)\n",
      "                all_records = pd.concat([all_records, new_stones])\n",
      "\n",
      "            if len(readds):\n",
      "                reindex(cur_day, READD, readds)\n",
      "                all_records = pd.concat([all_records, readds])\n",
      "                \n",
      "            # join the stones not removed w/ the current load to see what's changed\n",
      "            joined_px = pd.merge(kept_stones, active, left_index = True, right_index = True)\n",
      "            # see if any prices have changed\n",
      "            #print joined_px[joined_px.Price_x != joined_px.Price_y].index\n",
      "            #print active.index\n",
      "            px_changes = active.loc[joined_px[joined_px.Price_x != joined_px.Price_y].index]\n",
      "            if len(px_changes):\n",
      "                reindex(cur_day, PRICE_CHANGE, px_changes)\n",
      "                all_records = pd.concat([all_records, px_changes])\n",
      "\n",
      "            # see what's been removed\n",
      "            removals = prev.loc[prev.index - active.index]\n",
      "            if len(removals):\n",
      "                reindex(cur_day, REMOVE, removals)\n",
      "                all_records = pd.concat([all_records, removals])\n",
      "                \n",
      "            print '{0} new stones, {1} removals, {2} price changes, {3} readds'.format(\n",
      "                len(new_stones), len(removals), len(px_changes), len(readds))\n",
      "        else:\n",
      "            # create all_records w/ multiindex: event_date, event_type, certnum\n",
      "            all_records = active.copy()\n",
      "            reindex(cur_day, ADD, all_records)\n",
      "        prev = active\n",
      "        print 'took {0}'.format(datetime.now() - now)\n",
      "    return all_records\n",
      "\n",
      "records = build_cache()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "loading file for 20130303... Dupe filtering: dropped 2 duplicate of 9 rows\n",
        "took 0:00:00.006688\n",
        "loading file for 20130304... Dupe filtering: dropped 2 duplicate of 9 rows\n",
        "3 new stones, 3 removals, 1 price changes, 0 readds\n",
        "took 0:00:00.018216\n",
        "loading file for 20130305... Dupe filtering: dropped 2 duplicate of 10 rows\n",
        "MultiIndex\n",
        "[]\n",
        "0 new stones, 1 removals, 1 price changes, 2 readds\n",
        "took 0:00:00.024234\n"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "records.xs(REMOVE, level='event_type')[['LotNum', 'Price']]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th>LotNum</th>\n",
        "      <th>Price</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>CertNum</th>\n",
        "      <th>Owner</th>\n",
        "      <th>event_day</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>2136069993</th>\n",
        "      <th>CHINADIA</th>\n",
        "      <th>2013-03-04</th>\n",
        "      <td> 43640013</td>\n",
        "      <td> 10168</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2146438809</th>\n",
        "      <th>CHINADIA</th>\n",
        "      <th>2013-03-04</th>\n",
        "      <td> 43640014</td>\n",
        "      <td>  3015</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2151095680</th>\n",
        "      <th>CHINADIA1</th>\n",
        "      <th>2013-03-04</th>\n",
        "      <td> 43640016</td>\n",
        "      <td>  4830</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5146226160</th>\n",
        "      <th>CHINADIA</th>\n",
        "      <th>2013-03-05</th>\n",
        "      <td> 43640019</td>\n",
        "      <td>  7875</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 118,
       "text": [
        "                                   LotNum  Price\n",
        "CertNum    Owner     event_day                  \n",
        "2136069993 CHINADIA  2013-03-04  43640013  10168\n",
        "2146438809 CHINADIA  2013-03-04  43640014   3015\n",
        "2151095680 CHINADIA1 2013-03-04  43640016   4830\n",
        "5146226160 CHINADIA  2013-03-05  43640019   7875"
       ]
      }
     ],
     "prompt_number": 118
    }
   ],
   "metadata": {}
  }
 ]
}