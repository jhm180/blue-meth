{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import rapnet_loader as rl\n",
      "# uncomment and run the following line to see the source for rapnet_loader\n",
      "%load /home/oliver/ipynb/rapnet_loader.py\n",
      "\n",
      "#ADD = 1\n",
      "#REMOVE = 2\n",
      "#READD = 3\n",
      "#PRICE_CHANGE = 4\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from os import path\n",
      "import glob\n",
      "import re\n",
      "from datetime import datetime,date,timedelta\n",
      "from pandas.io.parsers import read_csv\n",
      "import numpy as np\n",
      "from itertools import repeat\n",
      "import sys\n",
      "import traceback\n",
      "import gc\n",
      "from pandas.io.pytables import HDFStore\n",
      "import shutil\n",
      "\n",
      "ADD = 1\n",
      "REMOVE = 2\n",
      "READD = 3\n",
      "PRICE_CHANGE = 4\n",
      "\n",
      "DATA_PATH = '/home/oliver/rapnet_data'\n",
      "CACHE_PATH = '/home/oliver/rapnet_cache'\n",
      "COL_NAMES = ['LotNum', 'Owner', 'Shape', 'Carat', 'Color', 'Clarity', 'Cut Grade', 'Price', 'PctRap',\n",
      " 'Cert', 'Depth', 'Table', 'Girdle', 'Culet', 'Polish', 'Sym', 'Fluor', 'Meas', 'Comment',\n",
      " 'NumStones', 'CertNum', 'StockNum', 'Make', 'Date', 'City', 'State', 'Country', 'Image']\n",
      "\n",
      "def read_daily_file(file_date):\n",
      "    compression = None\n",
      "    daily_file = path.join(DATA_PATH, 'Rapnet_{0}_Main.csv'.format(file_date))\n",
      "    if not path.exists(daily_file):\n",
      "        daily_file = daily_file + '.gz'\n",
      "        compression = \"gzip\"\n",
      "    if not path.exists(daily_file):\n",
      "        return []\n",
      "    try:\n",
      "        return read_csv(daily_file, compression = compression, names = COL_NAMES, header = 0,\n",
      "                    #dtype = {'Owner':str, 'CertNum':str},\n",
      "                    engine = 'python')\n",
      "    except EOFError:\n",
      "        print 'bad file {0}; renaming'.format(daily_file)\n",
      "        shutil.move(daily_file, daily_file + '.bad')\n",
      "        return []\n",
      "\n",
      "# don't load fake cert nums\n",
      "bad_certs = { '123456789', '', '1234567890', '0' }\n",
      "\n",
      "def days_on_market(df):\n",
      "    # grp here is all rows matching the same (owner,certnum) key\n",
      "    # here we arrange by date\n",
      "    intervals = []\n",
      "    # the all_df frame is already sorted by date, so no need to re-sort here if it's the same df\n",
      "    #df.sortlevel(level='event_day', inplace=True)\n",
      "    grpd = df.groupby(level=['Owner','CertNum'])\n",
      "    t1 = datetime.now()\n",
      "    for grpname, vals in grpd:\n",
      "        # loop for each event_type/date entry in this stone's dataframe\n",
      "        # name is the original multiindex of the row prior to the groupby operation\n",
      "        for name, valseries in vals.iterrows(): \n",
      "            # looks back through all events for a given stone and generates\n",
      "            # a new row for each interval on the market, indexed by certnum and owner\n",
      "            # (the group name, or label\n",
      "            et = name[0]\n",
      "            #print et\n",
      "            if et == ADD or et == READD:\n",
      "                date_added = name[3] \n",
      "            elif et == REMOVE:\n",
      "                if date_added != None:\n",
      "                    date_removed = name[3]\n",
      "                    intervals.append([date_added, date_removed, grpname])\n",
      "                                    #index = pd.MultiIndex(levels = [grpname[0], grpname[1], date_removed])))\n",
      "\n",
      "    tuples = [iv[2] for iv in intervals]\n",
      "    return pd.DataFrame([iv[:2] for iv in intervals], index = pd.MultiIndex.from_tuples(tuples), columns = ['added','removed'])\n",
      "\n",
      "def filter_data(df):\n",
      "    indices = []\n",
      "    for k, grpdf in df.groupby(['Owner','CertNum']):\n",
      "        if len(grpdf) == 1:\n",
      "            indices.append(grpdf.index[0])\n",
      "        elif not k[1] in bad_certs:\n",
      "            # we were dropping a lot of dupes, so instead i'm\n",
      "            # just adding the one w/ the highest lot num\n",
      "            indices.append(grpdf.LotNum.idxmax())\n",
      "    nodupes = df.loc[indices]\n",
      "    #dupes = df.loc[df.index - indices]\n",
      "    if len(df) - len(nodupes) > 0:\n",
      "        print 'Filtering: dropped {0} of {1} rows'.format(len(df) - len(nodupes), len(df))\n",
      "    stones = nodupes[[c for c in COL_NAMES if not c in {'CertNum','Owner'}]]\n",
      "    stones.index = pd.MultiIndex.from_tuples([(owner, cert) for owner, cert in nodupes[['Owner','CertNum']].values],\n",
      "                                             names = ['Owner', 'CertNum'])\n",
      "    return stones\n",
      "\n",
      "def cache_records(records, active, file_date):  \n",
      "    now = datetime.now()\n",
      "    atts = {\n",
      "          'file_date': file_date,\n",
      "          'records': records,\n",
      "          'active':active\n",
      "        }\n",
      "    pd.Series(atts).to_pickle(path.join(CACHE_PATH, 'rapnet.pkl'))\n",
      "    print 'cache write took {0}'.format(datetime.now() - now)\n",
      "    #s = HDFStore(path.join(CACHE_PATH, 'rapnet.h5'), complevel=9, complib='blosc')\n",
      "    #s['records'] = records\n",
      "    #s['active'] = active\n",
      "    #s['attributes'] = pd.Series({'file_date':file_date})\n",
      "    \n",
      "def load_cache():\n",
      "    now = datetime.now()\n",
      "    #h5path = path.join(CACHE_PATH, 'rapnet.h5')\n",
      "    #if path.exists(h5path):\n",
      "    #    s = H5Store(h5path)\n",
      "    #    atts = s['attributes'].to_dict()\n",
      "    #    return s['records'], s['active'], atts['file_date']\n",
      "    pklpath = path.join(CACHE_PATH, 'rapnet.pkl')\n",
      "    if path.exists(pklpath):\n",
      "        s = pd.read_pickle(pklpath).to_dict()\n",
      "        print 'cache load took {0}'.format(datetime.now() - now)\n",
      "        return s['records'], s['active'], s['file_date']\n",
      "    return [], [], []\n",
      "\n",
      "def gen_file_dates(start_day):\n",
      "    oneday = timedelta(1)\n",
      "    today = datetime.today()\n",
      "    #today = datetime.strptime('20130304', '%Y%m%d')\n",
      "    cur_day = start_day\n",
      "    while cur_day <= today:\n",
      "        yield cur_day\n",
      "        cur_day = cur_day + oneday\n",
      "        \n",
      "def get_latest(grp):\n",
      "    grp.sortlevel(level='event_day', inplace=True, ascending = False)\n",
      "    return grp.iloc[0]\n",
      "\n",
      "def build_cache():\n",
      "    all_records, prev, prev_day = load_cache()\n",
      "    first_day = datetime(2013,3,3)\n",
      "    if prev_day:\n",
      "        first_day = datetime.strptime(prev_day, '%Y%m%d') + timedelta(1)\n",
      "        print 'build_cache starting after previous load date', prev_day\n",
      "    i = 0\n",
      "    def reindex(idx_day, idx_event, df):\n",
      "        df.index = pd.MultiIndex.from_tuples([(idx_event, owner, cert, idx_day) for owner, cert in df.index],\n",
      "                                             names = ['event_type','Owner','CertNum','event_day'])\n",
      "            \n",
      "    file_date = None\n",
      "    for cur_day in gen_file_dates(first_day):\n",
      "        file_date = cur_day.strftime('%Y%m%d')\n",
      "        now = datetime.now()\n",
      "        df = read_daily_file(file_date)\n",
      "        if len(df) == 0:\n",
      "            continue\n",
      "        print 'processing file for {0}...'.format(file_date),\n",
      "        active = filter_data(df)\n",
      "        if len(prev):\n",
      "            new_stones = active.loc[active.index - prev.index]\n",
      "            kept_stones = prev.loc[active.index & prev.index]\n",
      "            readds = []\n",
      "            if len(new_stones):           \n",
      "                # cross section of data - we only want things were previously \n",
      "                # removed, to see if any of the new_stones are actually re-adds\n",
      "                all_removals = []\n",
      "                try:\n",
      "                    all_removals = all_records.xs(REMOVE, level = 'event_type')\n",
      "                except KeyError:\n",
      "                    pass\n",
      "                if len(all_removals):\n",
      "                    prev_removals = all_removals.groupby(level=['Owner','CertNum']).apply(get_latest)\n",
      "                    if len(prev_removals):\n",
      "                        readds = new_stones.loc[new_stones.index & prev_removals.index]\n",
      "                        if len(readds):\n",
      "                            newonly = new_stones.index - readds.index\n",
      "                            if len(newonly):\n",
      "                                new_stones = new_stones.loc[newonly]\n",
      "                            else:\n",
      "                                new_stones = []\n",
      "\n",
      "            if len(new_stones):\n",
      "                reindex(cur_day, ADD, new_stones)\n",
      "                all_records = pd.concat([all_records, new_stones])\n",
      "\n",
      "            if len(readds):\n",
      "                reindex(cur_day, READD, readds)\n",
      "                all_records = pd.concat([all_records, readds])\n",
      "                \n",
      "            # join the stones not removed w/ the current load to see what's changed\n",
      "            joined_px = pd.merge(kept_stones, active, left_index = True, right_index = True)\n",
      "\n",
      "            # see if any prices have changed\n",
      "            px_changes = active.loc[joined_px[(joined_px.Price_x != joined_px.Price_y) & ~(np.isnan(joined_px.Price_y))].index]\n",
      "            if len(px_changes):\n",
      "                reindex(cur_day, PRICE_CHANGE, px_changes)\n",
      "                all_records = pd.concat([all_records, px_changes])\n",
      "\n",
      "            # see what's been removed\n",
      "            removals = prev.loc[prev.index - active.index]\n",
      "            if len(removals):\n",
      "                reindex(cur_day, REMOVE, removals)\n",
      "                all_records = pd.concat([all_records, removals])\n",
      "                \n",
      "            print '{0} new stones, {1} removals, {2} price changes, {3} readds'.format(\n",
      "                len(new_stones), len(removals), len(px_changes), len(readds))\n",
      "        else:\n",
      "            # create all_records w/ multiindex: event_date, event_type, certnum\n",
      "            all_records = active.copy()\n",
      "            reindex(cur_day, ADD, all_records)\n",
      "        cache_records(all_records, active, file_date)\n",
      "        prev = active\n",
      "        gc.collect()\n",
      "        print 'took {0}'.format(datetime.now() - now)\n",
      "    return all_records, prev, file_date\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "   build_cache()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from os import path\n",
      "import glob\n",
      "import re\n",
      "from datetime import datetime,date,timedelta\n",
      "from pandas.io.parsers import read_csv\n",
      "import numpy as np\n",
      "from itertools import repeat\n",
      "import sys\n",
      "import traceback\n",
      "import gc\n",
      "from pandas.io.pytables import HDFStore\n",
      "import shutil\n",
      "\n",
      "ADD = 1\n",
      "REMOVE = 2\n",
      "READD = 3\n",
      "PRICE_CHANGE = 4\n",
      "\n",
      "DATA_PATH = '/home/oliver/rapnet_data'\n",
      "CACHE_PATH = '/home/oliver/rapnet_cache'\n",
      "COL_NAMES = ['LotNum', 'Owner', 'Shape', 'Carat', 'Color', 'Clarity', 'Cut Grade', 'Price', 'PctRap',\n",
      " 'Cert', 'Depth', 'Table', 'Girdle', 'Culet', 'Polish', 'Sym', 'Fluor', 'Meas', 'Comment',\n",
      " 'NumStones', 'CertNum', 'StockNum', 'Make', 'Date', 'City', 'State', 'Country', 'Image']\n",
      "\n",
      "def read_daily_file(file_date):\n",
      "    compression = None\n",
      "    daily_file = path.join(DATA_PATH, 'Rapnet_{0}_Main.csv'.format(file_date))\n",
      "    if not path.exists(daily_file):\n",
      "        daily_file = daily_file + '.gz'\n",
      "        compression = \"gzip\"\n",
      "    if not path.exists(daily_file):\n",
      "        return []\n",
      "    try:\n",
      "        return read_csv(daily_file, compression = compression, names = COL_NAMES, header = 0,\n",
      "                    #dtype = {'Owner':str, 'CertNum':str},\n",
      "                    engine = 'python')\n",
      "    except EOFError:\n",
      "        print 'bad file {0}; renaming'.format(daily_file)\n",
      "        shutil.move(daily_file, daily_file + '.bad')\n",
      "        return []\n",
      "\n",
      "# don't load fake cert nums\n",
      "bad_certs = { '123456789', '', '1234567890', '0' }\n",
      "\n",
      "def days_on_market(df):\n",
      "    # grp here is all rows matching the same (owner,certnum) key\n",
      "    # here we arrange by date\n",
      "    intervals = []\n",
      "    # the all_df frame is already sorted by date, so no need to re-sort here if it's the same df\n",
      "    #df.sortlevel(level='event_day', inplace=True)\n",
      "    grpd = df.groupby(level=['Owner','CertNum'])\n",
      "    t1 = datetime.now()\n",
      "    for grpname, vals in grpd:\n",
      "        # loop for each event_type/date entry in this stone's dataframe\n",
      "        # name is the original multiindex of the row prior to the groupby operation\n",
      "        for name, valseries in vals.iterrows(): \n",
      "            # looks back through all events for a given stone and generates\n",
      "            # a new row for each interval on the market, indexed by certnum and owner\n",
      "            # (the group name, or label\n",
      "            et = name[0]\n",
      "            #print et\n",
      "            if et == ADD or et == READD:\n",
      "                date_added = name[3] \n",
      "            elif et == REMOVE:\n",
      "                if date_added != None:\n",
      "                    date_removed = name[3]\n",
      "                    intervals.append([date_added, date_removed, grpname])\n",
      "                                    #index = pd.MultiIndex(levels = [grpname[0], grpname[1], date_removed])))\n",
      "\n",
      "    tuples = [iv[2] for iv in intervals]\n",
      "    return pd.DataFrame([iv[:2] for iv in intervals], index = pd.MultiIndex.from_tuples(tuples), columns = ['added','removed'])\n",
      "\n",
      "def filter_data(df):\n",
      "    indices = []\n",
      "    for k, grpdf in df.groupby(['Owner','CertNum']):\n",
      "        if len(grpdf) == 1:\n",
      "            indices.append(grpdf.index[0])\n",
      "        elif not k[1] in bad_certs:\n",
      "            # we were dropping a lot of dupes, so instead i'm\n",
      "            # just adding the one w/ the highest lot num\n",
      "            indices.append(grpdf.LotNum.idxmax())\n",
      "    nodupes = df.loc[indices]\n",
      "    #dupes = df.loc[df.index - indices]\n",
      "    if len(df) - len(nodupes) > 0:\n",
      "        print 'Filtering: dropped {0} of {1} rows'.format(len(df) - len(nodupes), len(df))\n",
      "    stones = nodupes[[c for c in COL_NAMES if not c in {'CertNum','Owner'}]]\n",
      "    stones.index = pd.MultiIndex.from_tuples([(owner, cert) for owner, cert in nodupes[['Owner','CertNum']].values],\n",
      "                                             names = ['Owner', 'CertNum'])\n",
      "    return stones\n",
      "\n",
      "def cache_records(records, active, file_date):  \n",
      "    now = datetime.now()\n",
      "    atts = {\n",
      "          'file_date': file_date,\n",
      "          'records': records,\n",
      "          'active':active\n",
      "        }\n",
      "    pd.Series(atts).to_pickle(path.join(CACHE_PATH, 'rapnet.pkl'))\n",
      "    print 'cache write took {0}'.format(datetime.now() - now)\n",
      "    #s = HDFStore(path.join(CACHE_PATH, 'rapnet.h5'), complevel=9, complib='blosc')\n",
      "    #s['records'] = records\n",
      "    #s['active'] = active\n",
      "    #s['attributes'] = pd.Series({'file_date':file_date})\n",
      "    \n",
      "def load_cache():\n",
      "    now = datetime.now()\n",
      "    #h5path = path.join(CACHE_PATH, 'rapnet.h5')\n",
      "    #if path.exists(h5path):\n",
      "    #    s = H5Store(h5path)\n",
      "    #    atts = s['attributes'].to_dict()\n",
      "    #    return s['records'], s['active'], atts['file_date']\n",
      "    pklpath = path.join(CACHE_PATH, 'rapnet.pkl')\n",
      "    if path.exists(pklpath):\n",
      "        s = pd.read_pickle(pklpath).to_dict()\n",
      "        print 'cache load took {0}'.format(datetime.now() - now)\n",
      "        return s['records'], s['active'], s['file_date']\n",
      "    return [], [], []\n",
      "\n",
      "def gen_file_dates(start_day):\n",
      "    oneday = timedelta(1)\n",
      "    today = datetime.today()\n",
      "    #today = datetime.strptime('20130304', '%Y%m%d')\n",
      "    cur_day = start_day\n",
      "    while cur_day <= today:\n",
      "        yield cur_day\n",
      "        cur_day = cur_day + oneday\n",
      "        \n",
      "def get_latest(grp):\n",
      "    grp.sortlevel(level='event_day', inplace=True, ascending = False)\n",
      "    return grp.iloc[0]\n",
      "\n",
      "def build_cache():\n",
      "    all_records, prev, prev_day = load_cache()\n",
      "    first_day = datetime(2013,3,3)\n",
      "    if prev_day:\n",
      "        first_day = datetime.strptime(prev_day, '%Y%m%d') + timedelta(1)\n",
      "        print 'build_cache starting after previous load date', prev_day\n",
      "    i = 0\n",
      "    def reindex(idx_day, idx_event, df):\n",
      "        df.index = pd.MultiIndex.from_tuples([(idx_event, owner, cert, idx_day) for owner, cert in df.index],\n",
      "                                             names = ['event_type','Owner','CertNum','event_day'])\n",
      "            \n",
      "    file_date = None\n",
      "    for cur_day in gen_file_dates(first_day):\n",
      "        file_date = cur_day.strftime('%Y%m%d')\n",
      "        now = datetime.now()\n",
      "        df = read_daily_file(file_date)\n",
      "        if len(df) == 0:\n",
      "            continue\n",
      "        print 'processing file for {0}...'.format(file_date),\n",
      "        active = filter_data(df)\n",
      "        if len(prev):\n",
      "            new_stones = active.loc[active.index - prev.index]\n",
      "            kept_stones = prev.loc[active.index & prev.index]\n",
      "            readds = []\n",
      "            if len(new_stones):           \n",
      "                # cross section of data - we only want things were previously \n",
      "                # removed, to see if any of the new_stones are actually re-adds\n",
      "                all_removals = []\n",
      "                try:\n",
      "                    all_removals = all_records.xs(REMOVE, level = 'event_type')\n",
      "                except KeyError:\n",
      "                    pass\n",
      "                if len(all_removals):\n",
      "                    prev_removals = all_removals.groupby(level=['Owner','CertNum']).apply(get_latest)\n",
      "                    if len(prev_removals):\n",
      "                        readds = new_stones.loc[new_stones.index & prev_removals.index]\n",
      "                        if len(readds):\n",
      "                            newonly = new_stones.index - readds.index\n",
      "                            if len(newonly):\n",
      "                                new_stones = new_stones.loc[newonly]\n",
      "                            else:\n",
      "                                new_stones = []\n",
      "\n",
      "            if len(new_stones):\n",
      "                reindex(cur_day, ADD, new_stones)\n",
      "                all_records = pd.concat([all_records, new_stones])\n",
      "\n",
      "            if len(readds):\n",
      "                reindex(cur_day, READD, readds)\n",
      "                all_records = pd.concat([all_records, readds])\n",
      "                \n",
      "            # join the stones not removed w/ the current load to see what's changed\n",
      "            joined_px = pd.merge(kept_stones, active, left_index = True, right_index = True)\n",
      "\n",
      "            # see if any prices have changed\n",
      "            px_changes = active.loc[joined_px[(joined_px.Price_x != joined_px.Price_y) & ~(np.isnan(joined_px.Price_y))].index]\n",
      "            if len(px_changes):\n",
      "                reindex(cur_day, PRICE_CHANGE, px_changes)\n",
      "                all_records = pd.concat([all_records, px_changes])\n",
      "\n",
      "            # see what's been removed\n",
      "            removals = prev.loc[prev.index - active.index]\n",
      "            if len(removals):\n",
      "                reindex(cur_day, REMOVE, removals)\n",
      "                all_records = pd.concat([all_records, removals])\n",
      "                \n",
      "            print '{0} new stones, {1} removals, {2} price changes, {3} readds'.format(\n",
      "                len(new_stones), len(removals), len(px_changes), len(readds))\n",
      "        else:\n",
      "            # create all_records w/ multiindex: event_date, event_type, certnum\n",
      "            all_records = active.copy()\n",
      "            reindex(cur_day, ADD, all_records)\n",
      "        cache_records(all_records, active, file_date)\n",
      "        prev = active\n",
      "        gc.collect()\n",
      "        print 'took {0}'.format(datetime.now() - now)\n",
      "    return all_records, prev, file_date\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "   build_cache()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from os import path\n",
      "import glob\n",
      "import re\n",
      "from datetime import datetime,date,timedelta,date_range\n",
      "from pandas.io.parsers import read_csv\n",
      "import numpy as np\n",
      "from itertools import repeat\n",
      "import sys\n",
      "import traceback\n",
      "import gc\n",
      "from pandas.io.pytables import HDFStore\n",
      "import shutil\n",
      "\n",
      "ADD = 1\n",
      "REMOVE = 2\n",
      "READD = 3\n",
      "PRICE_CHANGE = 4\n",
      "\n",
      "DATA_PATH = '/home/oliver/rapnet_data'\n",
      "CACHE_PATH = '/home/oliver/rapnet_cache'\n",
      "COL_NAMES = ['LotNum', 'Owner', 'Shape', 'Carat', 'Color', 'Clarity', 'Cut Grade', 'Price', 'PctRap',\n",
      " 'Cert', 'Depth', 'Table', 'Girdle', 'Culet', 'Polish', 'Sym', 'Fluor', 'Meas', 'Comment',\n",
      " 'NumStones', 'CertNum', 'StockNum', 'Make', 'Date', 'City', 'State', 'Country', 'Image']\n",
      "\n",
      "def read_daily_file(file_date):\n",
      "    compression = None\n",
      "    daily_file = path.join(DATA_PATH, 'Rapnet_{0}_Main.csv'.format(file_date))\n",
      "    if not path.exists(daily_file):\n",
      "        daily_file = daily_file + '.gz'\n",
      "        compression = \"gzip\"\n",
      "    if not path.exists(daily_file):\n",
      "        return []\n",
      "    try:\n",
      "        return read_csv(daily_file, compression = compression, names = COL_NAMES, header = 0,\n",
      "                    #dtype = {'Owner':str, 'CertNum':str},\n",
      "                    engine = 'python')\n",
      "    except EOFError:\n",
      "        print 'bad file {0}; renaming'.format(daily_file)\n",
      "        shutil.move(daily_file, daily_file + '.bad')\n",
      "        return []\n",
      "\n",
      "# don't load fake cert nums\n",
      "bad_certs = { '123456789', '', '1234567890', '0' }\n",
      "\n",
      "def filter_data(df):\n",
      "    indices = []\n",
      "    for k, grpdf in df.groupby(['Owner','CertNum']):\n",
      "        if len(grpdf) == 1:\n",
      "            indices.append(grpdf.index[0])\n",
      "        elif not k[1] in bad_certs:\n",
      "            # we were dropping a lot of dupes, so instead i'm\n",
      "            # just adding the one w/ the highest lot num\n",
      "            indices.append(grpdf.LotNum.idxmax())\n",
      "    nodupes = df.loc[indices]\n",
      "    #dupes = df.loc[df.index - indices]\n",
      "    if len(df) - len(nodupes) > 0:\n",
      "        print 'Filtering: dropped {0} of {1} rows'.format(len(df) - len(nodupes), len(df))\n",
      "    stones = nodupes[[c for c in COL_NAMES if not c in {'CertNum','Owner'}]]\n",
      "    stones.index = pd.MultiIndex.from_tuples([(owner, cert) for owner, cert in nodupes[['Owner','CertNum']].values],\n",
      "                                             names = ['Owner', 'CertNum'])\n",
      "    return stones\n",
      "\n",
      "def cache_records(records, active, file_date):  \n",
      "    now = datetime.now()\n",
      "    atts = {\n",
      "          'file_date': file_date,\n",
      "          'records': records,\n",
      "          'active':active\n",
      "        }\n",
      "    pd.Series(atts).to_pickle(path.join(CACHE_PATH, 'rapnet.pkl'))\n",
      "    print 'cache write took {0}'.format(datetime.now() - now)\n",
      "    #s = HDFStore(path.join(CACHE_PATH, 'rapnet.h5'), complevel=9, complib='blosc')\n",
      "    #s['records'] = records\n",
      "    #s['active'] = active\n",
      "    #s['attributes'] = pd.Series({'file_date':file_date})\n",
      "    \n",
      "def load_cache():\n",
      "    now = datetime.now()\n",
      "    #h5path = path.join(CACHE_PATH, 'rapnet.h5')\n",
      "    #if path.exists(h5path):\n",
      "    #    s = H5Store(h5path)\n",
      "    #    atts = s['attributes'].to_dict()\n",
      "    #    return s['records'], s['active'], atts['file_date']\n",
      "    pklpath = path.join(CACHE_PATH, 'rapnet.pkl')\n",
      "    if path.exists(pklpath):\n",
      "        s = pd.read_pickle(pklpath).to_dict()\n",
      "        print 'cache load took {0}'.format(datetime.now() - now)\n",
      "        return s['records'], s['active'], s['file_date']\n",
      "    return [], [], []\n",
      "\n",
      "def gen_file_dates(start_day):\n",
      "    oneday = timedelta(1)\n",
      "    today = datetime.today()\n",
      "    #today = datetime.strptime('20130304', '%Y%m%d')\n",
      "    cur_day = start_day\n",
      "    while cur_day <= today:\n",
      "        yield cur_day\n",
      "        cur_day = cur_day + oneday\n",
      "        \n",
      "def get_latest(grp):\n",
      "    grp.sortlevel(level='event_day', inplace=True, ascending = False)\n",
      "    return grp.iloc[0]\n",
      "\n",
      "def build_cache():\n",
      "    all_records, prev, prev_day = load_cache()\n",
      "    first_day = datetime(2013,3,3)\n",
      "    if prev_day:\n",
      "        first_day = datetime.strptime(prev_day, '%Y%m%d') + timedelta(1)\n",
      "        print 'build_cache starting after previous load date', prev_day\n",
      "    i = 0\n",
      "    def reindex(idx_day, idx_event, df):\n",
      "        df.index = pd.MultiIndex.from_tuples([(idx_event, owner, cert, idx_day) for owner, cert in df.index],\n",
      "                                             names = ['event_type','Owner','CertNum','event_day'])\n",
      "            \n",
      "    file_date = None\n",
      "    for cur_day in gen_file_dates(first_day):\n",
      "        file_date = cur_day.strftime('%Y%m%d')\n",
      "        now = datetime.now()\n",
      "        df = read_daily_file(file_date)\n",
      "        if len(df) == 0:\n",
      "            continue\n",
      "        print 'processing file for {0}...'.format(file_date),\n",
      "        active = filter_data(df)\n",
      "        if len(prev):\n",
      "            new_stones = active.loc[active.index - prev.index]\n",
      "            kept_stones = prev.loc[active.index & prev.index]\n",
      "            readds = []\n",
      "            if len(new_stones):           \n",
      "                # cross section of data - we only want things were previously \n",
      "                # removed, to see if any of the new_stones are actually re-adds\n",
      "                all_removals = []\n",
      "                try:\n",
      "                    all_removals = all_records.xs(REMOVE, level = 'event_type')\n",
      "                except KeyError:\n",
      "                    pass\n",
      "                if len(all_removals):\n",
      "                    prev_removals = all_removals.groupby(level=['Owner','CertNum']).apply(get_latest)\n",
      "                    if len(prev_removals):\n",
      "                        readds = new_stones.loc[new_stones.index & prev_removals.index]\n",
      "                        if len(readds):\n",
      "                            newonly = new_stones.index - readds.index\n",
      "                            if len(newonly):\n",
      "                                new_stones = new_stones.loc[newonly]\n",
      "                            else:\n",
      "                                new_stones = []\n",
      "\n",
      "            if len(new_stones):\n",
      "                reindex(cur_day, ADD, new_stones)\n",
      "                all_records = pd.concat([all_records, new_stones])\n",
      "\n",
      "            if len(readds):\n",
      "                reindex(cur_day, READD, readds)\n",
      "                all_records = pd.concat([all_records, readds])\n",
      "                \n",
      "            # join the stones not removed w/ the current load to see what's changed\n",
      "            joined_px = pd.merge(kept_stones, active, left_index = True, right_index = True)\n",
      "\n",
      "            # see if any prices have changed\n",
      "            px_changes = active.loc[joined_px[(joined_px.Price_x != joined_px.Price_y) & ~(np.isnan(joined_px.Price_y))].index]\n",
      "            if len(px_changes):\n",
      "                reindex(cur_day, PRICE_CHANGE, px_changes)\n",
      "                all_records = pd.concat([all_records, px_changes])\n",
      "\n",
      "            # see what's been removed\n",
      "            removals = prev.loc[prev.index - active.index]\n",
      "            if len(removals):\n",
      "                reindex(cur_day, REMOVE, removals)\n",
      "                all_records = pd.concat([all_records, removals])\n",
      "                \n",
      "            print '{0} new stones, {1} removals, {2} price changes, {3} readds'.format(\n",
      "                len(new_stones), len(removals), len(px_changes), len(readds))\n",
      "        else:\n",
      "            # create all_records w/ multiindex: event_date, event_type, certnum\n",
      "            all_records = active.copy()\n",
      "            reindex(cur_day, ADD, all_records)\n",
      "        cache_records(all_records, active, file_date)\n",
      "        prev = active\n",
      "        gc.collect()\n",
      "        print 'took {0}'.format(datetime.now() - now)\n",
      "    return all_records, prev, file_date\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "   build_cache()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_df, current_df, file_date = rl.load_cache()\n",
      "# all_df is the whole database, with the event types you see in the above cell\n",
      "# the index is a multiindex with levels ('event_type','Owner','CertNum,'event_day')\n",
      "\n",
      "#this gets the latest removals:\n",
      "prev_removals = all_df.xs(rl.REMOVE, level='event_type').groupby(level=['Owner','CertNum']).apply(rl.get_latest)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "cache load took 0:00:14.488447\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(current_df[np.isnan(current_df.Price)]) / float(len(current_df))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "0.07577035106546068"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# 1 - 1.5ct round sells\n",
      "\n",
      "import pandas as pd\n",
      "from os import path\n",
      "import glob\n",
      "import re\n",
      "from datetime import datetime,date,timedelta\n",
      "from pandas.io.parsers import read_csv\n",
      "import numpy as np\n",
      "from itertools import repeat\n",
      "import sys\n",
      "import traceback\n",
      "import gc\n",
      "from pandas.io.pytables import HDFStore\n",
      "import shutil\n",
      "\n",
      "fluor_faint = ['Faint ', 'Faint Blue', 'Slight', 'Very Slight ', 'Slight Blue', 'Very Slight Blue'] #Do NOT delete spaces at end of items in this list\n",
      "fluor_none = ['None '] #Do NOT delete spaces at end of items in this list\n",
      "fluor_medium = ['Medium ', 'Medium Blue', 'Medium Yellow'] #Do NOT delete spaces at end of items in this list\n",
      "fluor_strong = ['Strong ', 'Strong Blue', 'Very Strong Blue', 'Very Strong '] #Do NOT delete spaces at end of items in this list\n",
      "\n",
      "groups = [\n",
      "        #[['D','E'],['IF','VVS1','VVS2'], 'G01'],\\\n",
      "        #[['F','G'],['IF','VVS1','VVS2'], 'G02'],\\\n",
      "        #[['H','I'],['IF','VVS1','VVS2'], 'G03'],\\\n",
      "        #[['J','K'],['IF','VVS1','VVS2'], 'G4'],\\\n",
      "        #[['D','E'],['VS1','VS2'], 'G05'],\\\n",
      "        [['F','G'],['VS1','VS2'], 'G06'],\\\n",
      "        [['H','I'],['VS1','VS2'], 'G07'],\\\n",
      "        #[['J','K'],['VS1','VS2'], 'G08'],\\\n",
      "        #[['D','E'],['SI1','SI2','SI3'], 'G09'],\\\n",
      "        [['F','G'],['SI1','SI2','SI3'], 'G10'],\\\n",
      "        [['H','I'],['SI1','SI2','SI3'], 'G11'],\\\n",
      "        #[['J','K'],['SI1','SI2','SI3'], 'G12'],\\\n",
      "        #[['D','E'],['I1','I2','I3'], 'G13'],\\\n",
      "        #[['F','G'],['I1','I2','I3'], 'G14'],\\\n",
      "        #[['H','I'],['I1','I2','I3'], 'G15'],\\\n",
      "        #[['J','K'],['I1','I2','I3'], 'G16'],\\\n",
      "        #[['F','G','H','I'],['VS1','VS2'], 'G06 + G07'],\\\n",
      "        #[['F','G','H','I'],['SI1','SI2','SI3'], 'G10 + G11'],\\\n",
      "        ]\n",
      "\n",
      "carat_bins = [\n",
      "              ['GIA', 'Round', 1.00, 1.05, 'EX'], \\\n",
      "              ['GIA', 'Round', 1.05, 1.10, 'EX'], \\\n",
      "              ['GIA', 'Round', 1.10, 1.15, 'EX'], \\\n",
      "              ['GIA', 'Round', 1.15, 1.20, 'EX'], \\\n",
      "              ['GIA', 'Round', 1.20, 1.25, 'EX'], \\\n",
      "              ['GIA', 'Round', 1.25, 1.30, 'EX'], \\\n",
      "              ['GIA', 'Round', 1.30, 1.35, 'EX'], \\\n",
      "              ['GIA', 'Round', 1.35, 1.40, 'EX'], \\\n",
      "              ['GIA', 'Round', 1.40, 1.45, 'EX'], \\\n",
      "              ['GIA', 'Round', 1.45, 1.50, 'EX'], \\\n",
      "              ['GIA', 'Round', 1.00, 1.05, 'VG'], \\\n",
      "              ['GIA', 'Round', 1.05, 1.10, 'VG'], \\\n",
      "              ['GIA', 'Round', 1.10, 1.15, 'VG'], \\\n",
      "              ['GIA', 'Round', 1.15, 1.20, 'VG'], \\\n",
      "              ['GIA', 'Round', 1.20, 1.25, 'VG'], \\\n",
      "              ['GIA', 'Round', 1.25, 1.30, 'VG'], \\\n",
      "              ['GIA', 'Round', 1.30, 1.35, 'VG'], \\\n",
      "              ['GIA', 'Round', 1.35, 1.40, 'VG'], \\\n",
      "              ['GIA', 'Round', 1.40, 1.45, 'VG'], \\\n",
      "              ['GIA', 'Round', 1.45, 1.50, 'VG'], \\\n",
      "              ['GIA', 'Round', 1.00, 1.05, 'GD'], \\\n",
      "              ['GIA', 'Round', 1.05, 1.10, 'GD'], \\\n",
      "              ['GIA', 'Round', 1.10, 1.15, 'GD'], \\\n",
      "              ['GIA', 'Round', 1.15, 1.20, 'GD'], \\\n",
      "              ['GIA', 'Round', 1.20, 1.25, 'GD'], \\\n",
      "              ['GIA', 'Round', 1.25, 1.30, 'GD'], \\\n",
      "              ['GIA', 'Round', 1.30, 1.35, 'GD'], \\\n",
      "              ['GIA', 'Round', 1.35, 1.40, 'GD'], \\\n",
      "              ['GIA', 'Round', 1.40, 1.45, 'GD'], \\\n",
      "              ['GIA', 'Round', 1.45, 1.50, 'GD'], \\\n",
      "             ]\n",
      "\n",
      "#sale_dates = pd.date_range(start='2013-03-03',end=(datetime.now()-timedelta(days=30)),freq='1D')\n",
      "\n",
      "removals = all_df.xs(rl.REMOVE, level='event_type').groupby(level=['Owner','CertNum']).apply(rl.get_latest)\n",
      "adds = all_df.xs(rl.ADD, level='event_type').groupby(level=['Owner','CertNum']).apply(rl.get_latest)\n",
      "\n",
      "prev_removals = removals.join(adds, on=['Owner','CertNum'])\n",
      "\n",
      "output = {\\\n",
      "            'Lab' : [], \\\n",
      "            'Color' : [], \\\n",
      "            'GroupTag' : [], \\\n",
      "            'Clarity' : [], \\\n",
      "            'WeightMin' : [], \\\n",
      "            'WeightMax' : [], \\\n",
      "            'Cut' : [], \\\n",
      "            'NumSales' : [], \\\n",
      "            'AvgTimeToSell' : [], \\\n",
      "            'MedianTimeToSell' : [], \\\n",
      "            'StdTimeToSell' : [], \\\n",
      "            'NumInventory' : [], \\\n",
      "            'AvgInventoryAge' : [], \\\n",
      "            'MedianInventoryAge' : [], \\\n",
      "            'StdInventoryAge' : [] \\\n",
      "            }\n",
      "\n",
      "for i in range(len(carat_bins)):\n",
      "    lab = carat_bins[i][0]\n",
      "    shape = carat_bins[i][1]\n",
      "    min_ct = carat_bins[i][2]\n",
      "    max_ct = carat_bins[i][3]\n",
      "    cut = carat_bins[i][4]\n",
      "    for j in range(len(groups)):\n",
      "        colors = groups[j][0]\n",
      "        clars = groups[j][1]\n",
      "        grouptag = groups[j][2]\n",
      "        df_temp = prev_removals[ \\\n",
      "                    (prev_removals['Lab'] == lab) \\\n",
      "                    & (prev_removals['Weight'] >= min_ct) \\\n",
      "                    & (prev_removals['Weight'] <= max_ct) \\\n",
      "                    & (prev_removals['Color'].isin(colors)) \\\n",
      "                    & (prev_removals['Clarity'].isin(clars)) \\\n",
      "                    & (prev_removals['Cut'] == cut) \\\n",
      "                    & (prev_removals['Fluor'].isin(fluor_none)) \\\n",
      "                    & (prev_removals['Country'] = 'USA') \\\n",
      "                    & (prev_removals['EventDate'] <= (datetime.now()-timedelta(days=30))) \\\n",
      "                    ]\n",
      "\n",
      "        if len(df_temp):\n",
      "            numsales = len(df_temp)\n",
      "            avgtimetosell = np.mean(prev_removals['Price']*prev_removals['\n",
      "        except KeyError: \n",
      "\n",
      "#        for k in range(len(colors)):\n",
      "#            for l in range(len(clars)):\n",
      "#                color = colors[k]\n",
      "#                clar = clars[l]\n",
      "                \n",
      "        output['Lab'].append(lab)\n",
      "        output['GroupTag'].append(grouptag)\n",
      "        output['Color'].append(colors)\n",
      "        output['Clarity'].append(clars)\n",
      "        output['WeightMin'].append(min_ct)\n",
      "        output['WeightMax'].append(max_ct)\n",
      "        output['Cut'].append(cut)\n",
      "        output['NumSales'].append(numsales)\n",
      "        output['AvgTimeToSell'].append(avgtimetosell)\n",
      "        output['MedianTimeToSell'].append(medtimetosell)\n",
      "        output['StdTimeToSell'].append(stdtimetosell)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Exception",
       "evalue": "Level <class 'pandas.tseries.index.DatetimeIndex'>\n[2013-03-03 00:00:00, ..., 2013-10-27 00:00:00]\nLength: 239, Freq: D, Timezone: None not found",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-12-9fcb54c4a5d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0msale_dates\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdate_range\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'2013-03-03'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mtimedelta\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'1D'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mprev_removals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREMOVE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msale_dates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Owner'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'CertNum'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_latest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprev_removals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/oliver/env/pydata/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mxs\u001b[1;34m(self, key, axis, level, copy)\u001b[0m\n\u001b[0;32m   2333\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2335\u001b[1;33m             \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_ax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2337\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/oliver/env/pydata/lib/python2.7/site-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36mget_loc_level\u001b[1;34m(self, key, level)\u001b[0m\n\u001b[0;32m   2320\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_drop_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2322\u001b[1;33m         \u001b[0mlevel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_level_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2324\u001b[0m         \u001b[1;31m# kludge for #1796\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/oliver/env/pydata/lib/python2.7/site-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36m_get_level_number\u001b[1;34m(self, level)\u001b[0m\n\u001b[0;32m   1540\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1541\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1542\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Level %s not found'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1543\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1544\u001b[0m                 \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mException\u001b[0m: Level <class 'pandas.tseries.index.DatetimeIndex'>\n[2013-03-03 00:00:00, ..., 2013-10-27 00:00:00]\nLength: 239, Freq: D, Timezone: None not found"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dom = rl.days_on_market(filtered_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "prev_removals = pd.concat([removals,adds], axis=1, join='inner', join_axes=[removals.index])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AssertionError",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-44-572487d4d7dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprev_removals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mremovals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0madds\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'inner'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin_axes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'CertNum'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Owner'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/home/oliver/env/pydata/lib/python2.7/site-packages/pandas/tools/merge.pyc\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity)\u001b[0m\n\u001b[0;32m    876\u001b[0m                        \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 878\u001b[1;33m                        verify_integrity=verify_integrity)\n\u001b[0m\u001b[0;32m    879\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/oliver/env/pydata/lib/python2.7/site-packages/pandas/tools/merge.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity)\u001b[0m\n\u001b[0;32m    952\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverify_integrity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mverify_integrity\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 954\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_new_axes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    955\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/oliver/env/pydata/lib/python2.7/site-packages/pandas/tools/merge.pyc\u001b[0m in \u001b[0;36m_get_new_axes\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin_axes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mndim\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1134\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m             \u001b[1;31m# ufff...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAssertionError\u001b[0m: "
       ]
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test = all_df.xs((rl.ADD,'2013-09-10'), level=['event_type','event_day']).groupby(level=['Owner','CertNum']).apply(rl.get_latest)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Exception",
       "evalue": "Level event_day not found",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-33-6df2a87133d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mADD\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'2013-09-10'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'event_type'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'event_day'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Owner'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'CertNum'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_latest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/home/oliver/env/pydata/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_intercept_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 331\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_python_apply_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/oliver/env/pydata/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36m_python_apply_general\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_python_apply_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 334\u001b[1;33m         \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmutated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         return self._wrap_applied_output(keys, values,\n",
        "\u001b[1;32m/home/oliver/env/pydata/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, data, axis, keep_internal)\u001b[0m\n\u001b[0;32m    628\u001b[0m             \u001b[1;31m# group might be modified\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m             \u001b[0mgroup_axes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_axes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_is_indexed_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup_axes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m                 \u001b[0mmutated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/oliver/env/pydata/lib/python2.7/site-packages/pandas/core/groupby.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(g)\u001b[0m\n\u001b[0;32m    328\u001b[0m         \"\"\"\n\u001b[0;32m    329\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_intercept_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_apply_general\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/oliver/src/blue-meth/ipynb/rapnet_loader.pyc\u001b[0m in \u001b[0;36mget_latest\u001b[1;34m(grp)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_latest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[0mgrp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msortlevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'event_day'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/oliver/env/pydata/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36msortlevel\u001b[1;34m(self, level, axis, ascending, inplace)\u001b[0m\n\u001b[0;32m   3315\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'can only sort by level with a hierarchical index'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3317\u001b[1;33m         \u001b[0mnew_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthe_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msortlevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3319\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_mixed_type\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/oliver/env/pydata/lib/python2.7/site-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36msortlevel\u001b[1;34m(self, level, ascending)\u001b[0m\n\u001b[0;32m   2064\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2065\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2066\u001b[1;33m         \u001b[0mlevel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_level_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2067\u001b[0m         \u001b[0mprimary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2068\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/oliver/env/pydata/lib/python2.7/site-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36m_get_level_number\u001b[1;34m(self, level)\u001b[0m\n\u001b[0;32m   1540\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1541\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1542\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Level %s not found'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1543\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1544\u001b[0m                 \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mException\u001b[0m: Level event_day not found"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print dom.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                                 added             removed\n",
        "100 DEM 1152613094 2013-10-15 00:00:00 2013-10-28 00:00:00\n",
        "        1156533521 2013-07-18 00:00:00 2013-07-19 00:00:00\n",
        "        1156551609 2013-10-08 00:00:00 2013-10-11 00:00:00\n",
        "        1156568789 2013-08-02 00:00:00 2013-08-15 00:00:00\n",
        "        1156655500 2013-09-18 00:00:00 2013-09-25 00:00:00\n",
        "        17428598   2013-09-25 00:00:00 2013-10-02 00:00:00\n",
        "        2151535567 2013-07-19 00:00:00 2013-08-08 00:00:00\n",
        "        2151589445 2013-08-12 00:00:00 2013-08-14 00:00:00\n",
        "        2151589445 2013-08-26 00:00:00 2013-08-27 00:00:00\n",
        "        2151606358 2013-10-02 00:00:00 2013-10-22 00:00:00\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyboardInterrupt",
       "evalue": "",
       "output_type": "pyerr",
       "traceback": ""
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Traceback (most recent call last):\n",
        "  File \"/home/oliver/env/pydata/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 759, in structured_traceback\n",
        "    records = _fixed_getinnerframes(etb, context, tb_offset)\n",
        "  File \"/home/oliver/env/pydata/lib/python2.7/site-packages/IPython/core/ultratb.py\", line 242, in _fixed_getinnerframes\n",
        "    records  = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
        "  File \"/usr/lib/python2.7/inspect.py\", line 1043, in getinnerframes\n",
        "    framelist.append((tb.tb_frame,) + getframeinfo(tb, context))\n",
        "  File \"/usr/lib/python2.7/inspect.py\", line 1003, in getframeinfo\n",
        "    filename = getsourcefile(frame) or getfile(frame)\n",
        "  File \"/usr/lib/python2.7/inspect.py\", line 454, in getsourcefile\n",
        "    if hasattr(getmodule(object, filename), '__loader__'):\n",
        "  File \"/usr/lib/python2.7/inspect.py\", line 490, in getmodule\n",
        "    for modname, module in sys.modules.items():\n",
        "KeyboardInterrupt\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "ERROR: Internal Python error in the inspect module.\n",
        "Below is the traceback from this internal error.\n",
        "\n",
        "\n",
        "Unfortunately, your original traceback can not be constructed.\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(all_df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2969552\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}