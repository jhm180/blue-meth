{
 "metadata": {
  "name": "",
  "signature": "sha256:b560ba8dd5d8bfa2a9730c1d7f215e16b02593d03a146331d7f20fa52833e1d0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load /home/oliver/ipynb/rapnet_loader.py"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from os import path\n",
      "from datetime import datetime, timedelta\n",
      "from pandas.io.parsers import read_csv\n",
      "import numpy as np\n",
      "import gc\n",
      "# from pandas.io.pytables import HDFStore\n",
      "import shutil\n",
      "\n",
      "ADD = 1\n",
      "REMOVE = 2\n",
      "READD = 3\n",
      "PRICE_CHANGE = 4\n",
      "\n",
      "DATA_PATH = '/home/oliver/rapnet_data'\n",
      "CACHE_PATH = '/home/oliver/rapnet_cache'\n",
      "COL_NAMES = ['LotNum', 'Owner', 'Shape', 'Carat', 'Color', 'Clarity', 'Cut Grade', 'Price', 'PctRap',\n",
      " 'Cert', 'Depth', 'Table', 'Girdle', 'Culet', 'Polish', 'Sym', 'Fluor', 'Meas', 'Comment',\n",
      " 'NumStones', 'CertNum', 'StockNum', 'Make', 'Date', 'City', 'State', 'Country', 'Image']\n",
      "\n",
      "# actual headers for new format:\n",
      "# Seller Name,RapNet Account ID,Name Code,Shape,Weight,Color,Clarity,Cut,Polish,\n",
      "# Symmetry,Fluorescence Color,Measurements,Lab,Certificate Number,Stock Number,\n",
      "# Price Per Carat,Price Percentage,Cash Price Per Carat,Cash Price Percentage,\n",
      "# Depth,Table,Girdle,Culet,Culet Size,Culet Condition,\n",
      "# City,State,Country,Certificate URL,Image URL,Depth Percent,Diamond ID\n",
      "\n",
      "COL_NAMES_20141205 = [\n",
      "'SellerName','RapnetAccount','Owner','Shape','Carat','Color','Clarity','Cut Grade','Polish',\n",
      "'Sym','Fluor','Meas','Cert','CertNum','StockNum',\n",
      "'Price','PctRap',\n",
      "'Depth','Table','Girdle','Culet','Culet Size','Culet Condition',\n",
      "'City','State','Country','Certificate URL','Image','Depth Percent','LotNum'\n",
      "]\n",
      "\n",
      "COL_NAMES_20141218 = [\n",
      "'SellerName','RapnetAccount','Owner','Shape','Carat','Color','Clarity','Cut Grade','Polish',\n",
      "'Sym','Fluor','Meas','Cert','CertNum','StockNum',\n",
      "'Price','PctRap','Cash Price Per Carat','Cash Price Percentage',\n",
      "'Depth','Table','Girdle','Culet','Culet Size','Culet Condition',\n",
      "'City','State','Country','Certificate URL','Image','Depth Percent','LotNum'\n",
      "]\n",
      "\n",
      "USE_COLS = ['LotNum', 'Owner', 'Shape', 'Carat', 'Color', 'Clarity', 'Cut Grade', 'Price', 'PctRap',\n",
      " 'Cert', 'Depth', 'Table', 'Girdle', 'Culet', 'Polish', 'Sym', 'Fluor', 'Meas',\n",
      " 'CertNum', 'StockNum', 'City', 'State', 'Country', 'Image']\n",
      "\n",
      "NEW_FMT_DATE = datetime(2014, 12, 4)\n",
      "NEW_FMT_DATE2 = datetime(2014, 12, 17)\n",
      "\n",
      "def read_daily_file(file_date, cur_day):\n",
      "    compression = None\n",
      "    daily_file = path.join(DATA_PATH, 'Rapnet_{0}_Main.csv'.format(file_date))\n",
      "    if not path.exists(daily_file):\n",
      "        daily_file = daily_file + '.gz'\n",
      "        compression = \"gzip\"\n",
      "    if not path.exists(daily_file):\n",
      "        return []\n",
      "    try:\n",
      "        if cur_day > NEW_FMT_DATE2:\n",
      "            print \"loading date {0} with new format 2\".format(file_date)\n",
      "            return read_csv(daily_file, compression = compression, names = COL_NAMES_20141218, header = 0,\n",
      "                        engine = 'python')[USE_COLS]\n",
      "        if cur_day > NEW_FMT_DATE:\n",
      "            print \"loading date {0} with new format\".format(file_date)\n",
      "            return read_csv(daily_file, compression = compression, names = COL_NAMES_20141205, header = 0,\n",
      "                        engine = 'python')[USE_COLS]\n",
      "        else:\n",
      "            return read_csv(daily_file, compression = compression, names = COL_NAMES, header = 0,\n",
      "                        engine = 'python')[USE_COLS]\n",
      "    except EOFError:\n",
      "        print 'bad file {0}; renaming'.format(daily_file)\n",
      "        shutil.move(daily_file, daily_file + '.bad')\n",
      "        return []\n",
      "\n",
      "# don't load fake cert nums\n",
      "bad_certs = { '123456789', '', '1234567890', '0' }\n",
      "\n",
      "def days_on_market(df):\n",
      "    # grp here is all rows matching the same (owner,certnum) key\n",
      "    # here we arrange by date\n",
      "    intervals = []\n",
      "    # the all_df frame is already sorted by date, so no need to re-sort here if it's the same df\n",
      "    #df.sortlevel(level='event_day', inplace=True)\n",
      "    grpd = df.groupby(level=['Owner','CertNum'])\n",
      "    for grpname, vals in grpd:\n",
      "        # loop for each event_type/date entry in this stone's dataframe\n",
      "        # name is the original multiindex of the row prior to the groupby operation\n",
      "        for name, valseries in vals.iterrows():\n",
      "            # looks back through all events for a given stone and generates\n",
      "            # a new row for each interval on the market, indexed by certnum and owner\n",
      "            # (the group name, or label\n",
      "            et = name[0]\n",
      "            #print et\n",
      "            if et == ADD or et == READD:\n",
      "                date_added = name[3]\n",
      "            elif et == REMOVE:\n",
      "                if date_added != None:\n",
      "                    date_removed = name[3]\n",
      "                    intervals.append([date_added, date_removed, grpname])\n",
      "                                    #index = pd.MultiIndex(levels = [grpname[0], grpname[1], date_removed])))\n",
      "\n",
      "    tuples = [iv[2] for iv in intervals]\n",
      "    return pd.DataFrame([iv[:2] for iv in intervals], index = pd.MultiIndex.from_tuples(tuples), columns = ['added','removed'])\n",
      "\n",
      "def filter_data(df):\n",
      "    indices = []\n",
      "    for k, grpdf in df.groupby(['Owner','CertNum']):\n",
      "        if len(grpdf) == 1:\n",
      "            indices.append(grpdf.index[0])\n",
      "        elif not k[1] in bad_certs:\n",
      "            # we were dropping a lot of dupes, so instead i'm\n",
      "            # just adding the one w/ the highest lot num\n",
      "            indices.append(grpdf.LotNum.idxmax())\n",
      "    nodupes = df.loc[indices]\n",
      "    #dupes = df.loc[df.index - indices]\n",
      "    if len(df) - len(nodupes) > 0:\n",
      "        print 'Filtering: dropped {0} of {1} rows'.format(len(df) - len(nodupes), len(df))\n",
      "    stones = nodupes[[c for c in USE_COLS if not c in {'CertNum','Owner'}]]\n",
      "    stones.index = pd.MultiIndex.from_tuples([(owner, cert) for owner, cert in nodupes[['Owner','CertNum']].values],\n",
      "                                             names = ['Owner', 'CertNum'])\n",
      "    return stones\n",
      "\n",
      "def cache_records(records, active, file_date):\n",
      "    now = datetime.now()\n",
      "    atts = {\n",
      "          'file_date': file_date,\n",
      "          'records': records,\n",
      "          'active':active\n",
      "        }\n",
      "    pd.Series(atts).to_pickle(path.join(CACHE_PATH, 'rapnet.pkl'))\n",
      "    print 'cache write took {0}'.format(datetime.now() - now)\n",
      "    #s = HDFStore(path.join(CACHE_PATH, 'rapnet.h5'), complevel=9, complib='blosc')\n",
      "    #s['records'] = records\n",
      "    #s['active'] = active\n",
      "    #s['attributes'] = pd.Series({'file_date':file_date})\n",
      "\n",
      "def load_cache():\n",
      "    now = datetime.now()\n",
      "    #h5path = path.join(CACHE_PATH, 'rapnet.h5')\n",
      "    #if path.exists(h5path):\n",
      "    #    s = H5Store(h5path)\n",
      "    #    atts = s['attributes'].to_dict()\n",
      "    #    return s['records'], s['active'], atts['file_date']\n",
      "    pklpath = path.join(CACHE_PATH, 'rapnet.pkl')\n",
      "    if path.exists(pklpath):\n",
      "        s = pd.read_pickle(pklpath).to_dict()\n",
      "        print 'cache load took {0}'.format(datetime.now() - now)\n",
      "        return s['records'], s['active'], s['file_date']\n",
      "    return [], [], []\n",
      "\n",
      "def gen_file_dates(start_day):\n",
      "    oneday = timedelta(1)\n",
      "    today = datetime.today()\n",
      "    #today = datetime.strptime('20130304', '%Y%m%d')\n",
      "    cur_day = start_day\n",
      "    while cur_day <= today:\n",
      "        yield cur_day\n",
      "        cur_day = cur_day + oneday\n",
      "\n",
      "def get_latest(grp):\n",
      "    grp.sortlevel(level='event_day', inplace=True, ascending = False)\n",
      "    return grp.iloc[0]\n",
      "\n",
      "def build_cache():\n",
      "    all_records, prev, prev_day = load_cache()\n",
      "    first_day = datetime(2013,3,3)\n",
      "    if prev_day:\n",
      "        first_day = datetime.strptime(prev_day, '%Y%m%d') + timedelta(1)\n",
      "        print 'build_cache starting after previous load date', prev_day\n",
      "    def reindex(idx_day, idx_event, df):\n",
      "        df.index = pd.MultiIndex.from_tuples([(idx_event, owner, cert, idx_day) for owner, cert in df.index],\n",
      "                                             names = ['event_type','Owner','CertNum','event_day'])\n",
      "\n",
      "    file_date = None\n",
      "    for cur_day in gen_file_dates(first_day):\n",
      "        file_date = cur_day.strftime('%Y%m%d')\n",
      "        now = datetime.now()\n",
      "        df = read_daily_file(file_date, cur_day)\n",
      "        if len(df) == 0:\n",
      "            continue\n",
      "        print 'processing file for {0}...'.format(file_date),\n",
      "        active = filter_data(df)\n",
      "        if len(prev):\n",
      "            new_stones = active.loc[active.index - prev.index]\n",
      "            kept_stones = prev.loc[active.index & prev.index]\n",
      "            readds = []\n",
      "            if len(new_stones):\n",
      "                # cross section of data - we only want things were previously\n",
      "                # removed, to see if any of the new_stones are actually re-adds\n",
      "                all_removals = []\n",
      "                try:\n",
      "                    all_removals = all_records.xs(REMOVE, level = 'event_type')\n",
      "                except KeyError:\n",
      "                    pass\n",
      "                if len(all_removals):\n",
      "                    prev_removals = all_removals.groupby(level=['Owner','CertNum']).apply(get_latest)\n",
      "                    if len(prev_removals):\n",
      "                        readds = new_stones.loc[new_stones.index & prev_removals.index]\n",
      "                        if len(readds):\n",
      "                            newonly = new_stones.index - readds.index\n",
      "                            if len(newonly):\n",
      "                                new_stones = new_stones.loc[newonly]\n",
      "                            else:\n",
      "                                new_stones = []\n",
      "\n",
      "            if len(new_stones):\n",
      "                reindex(cur_day, ADD, new_stones)\n",
      "                all_records = pd.concat([all_records, new_stones])\n",
      "\n",
      "            if len(readds):\n",
      "                reindex(cur_day, READD, readds)\n",
      "                all_records = pd.concat([all_records, readds])\n",
      "\n",
      "            # join the stones not removed w/ the current load to see what's changed\n",
      "            joined_px = pd.merge(kept_stones, active, left_index = True, right_index = True)\n",
      "\n",
      "            # see if any prices have changed\n",
      "            px_changes = active.loc[joined_px[(joined_px.Price_x != joined_px.Price_y) & ~(np.isnan(joined_px.Price_y))].index]\n",
      "            if len(px_changes):\n",
      "                reindex(cur_day, PRICE_CHANGE, px_changes)\n",
      "                all_records = pd.concat([all_records, px_changes])\n",
      "\n",
      "            # see what's been removed\n",
      "            removals = prev.loc[prev.index - active.index]\n",
      "            if len(removals):\n",
      "                reindex(cur_day, REMOVE, removals)\n",
      "                all_records = pd.concat([all_records, removals])\n",
      "\n",
      "            print '{0} new stones, {1} removals, {2} price changes, {3} readds'.format(\n",
      "                len(new_stones), len(removals), len(px_changes), len(readds))\n",
      "        else:\n",
      "            # create all_records w/ multiindex: event_date, event_type, certnum\n",
      "            all_records = active.copy()\n",
      "            reindex(cur_day, ADD, all_records)\n",
      "        cache_records(all_records, active, file_date)\n",
      "        prev = active\n",
      "        gc.collect()\n",
      "        print 'took {0}'.format(datetime.now() - now)\n",
      "    return all_records, prev, file_date\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    build_cache()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from os import path\n",
      "import glob\n",
      "import re\n",
      "from datetime import datetime,date,timedelta\n",
      "from pandas.io.parsers import read_csv\n",
      "import numpy as np\n",
      "from itertools import repeat\n",
      "import sys\n",
      "import traceback\n",
      "import gc\n",
      "from pandas.io.pytables import HDFStore\n",
      "import shutil\n",
      "\n",
      "ADD = 1\n",
      "REMOVE = 2\n",
      "READD = 3\n",
      "PRICE_CHANGE = 4\n",
      "\n",
      "DATA_PATH = '/home/oliver/rapnet_data'\n",
      "CACHE_PATH = '/home/oliver/rapnet_cache'\n",
      "COL_NAMES = ['LotNum', 'Owner', 'Shape', 'Carat', 'Color', 'Clarity', 'Cut Grade', 'Price', 'PctRap',\n",
      " 'Cert', 'Depth', 'Table', 'Girdle', 'Culet', 'Polish', 'Sym', 'Fluor', 'Meas', 'Comment',\n",
      " 'NumStones', 'CertNum', 'StockNum', 'Make', 'Date', 'City', 'State', 'Country', 'Image']\n",
      "\n",
      "def read_daily_file(file_date):\n",
      "    compression = None\n",
      "    daily_file = path.join(DATA_PATH, 'Rapnet_{0}_Main.csv'.format(file_date))\n",
      "    if not path.exists(daily_file):\n",
      "        daily_file = daily_file + '.gz'\n",
      "        compression = \"gzip\"\n",
      "    if not path.exists(daily_file):\n",
      "        return []\n",
      "    try:\n",
      "        return read_csv(daily_file, compression = compression, names = COL_NAMES, header = 0,\n",
      "                    #dtype = {'Owner':str, 'CertNum':str},\n",
      "                    engine = 'python')\n",
      "    except EOFError:\n",
      "        print 'bad file {0}; renaming'.format(daily_file)\n",
      "        shutil.move(daily_file, daily_file + '.bad')\n",
      "        return []\n",
      "\n",
      "# don't load fake cert nums\n",
      "bad_certs = { '123456789', '', '1234567890', '0' }\n",
      "\n",
      "def days_on_market(df):\n",
      "    # grp here is all rows matching the same (owner,certnum) key\n",
      "    # here we arrange by date\n",
      "    intervals = []\n",
      "    # the all_df frame is already sorted by date, so no need to re-sort here if it's the same df\n",
      "    #df.sortlevel(level='event_day', inplace=True)\n",
      "    grpd = df.groupby(level=['Owner','CertNum'])\n",
      "    t1 = datetime.now()\n",
      "    for grpname, vals in grpd:\n",
      "        # loop for each event_type/date entry in this stone's dataframe\n",
      "        # name is the original multiindex of the row prior to the groupby operation\n",
      "        for name, valseries in vals.iterrows(): \n",
      "            # looks back through all events for a given stone and generates\n",
      "            # a new row for each interval on the market, indexed by certnum and owner\n",
      "            # (the group name, or label\n",
      "            et = name[0]\n",
      "            #print et\n",
      "            if et == ADD or et == READD:\n",
      "                date_added = name[3] \n",
      "            elif et == REMOVE:\n",
      "                if date_added != None:\n",
      "                    date_removed = name[3]\n",
      "                    intervals.append([date_added, date_removed, grpname])\n",
      "                                    #index = pd.MultiIndex(levels = [grpname[0], grpname[1], date_removed])))\n",
      "\n",
      "    tuples = [iv[2] for iv in intervals]\n",
      "    return pd.DataFrame([iv[:2] for iv in intervals], index = pd.MultiIndex.from_tuples(tuples), columns = ['added','removed'])\n",
      "\n",
      "def filter_data(df):\n",
      "    indices = []\n",
      "    for k, grpdf in df.groupby(['Owner','CertNum']):\n",
      "        if len(grpdf) == 1:\n",
      "            indices.append(grpdf.index[0])\n",
      "        elif not k[1] in bad_certs:\n",
      "            # we were dropping a lot of dupes, so instead i'm\n",
      "            # just adding the one w/ the highest lot num\n",
      "            indices.append(grpdf.LotNum.idxmax())\n",
      "    nodupes = df.loc[indices]\n",
      "    #dupes = df.loc[df.index - indices]\n",
      "    if len(df) - len(nodupes) > 0:\n",
      "        print 'Filtering: dropped {0} of {1} rows'.format(len(df) - len(nodupes), len(df))\n",
      "    stones = nodupes[[c for c in COL_NAMES if not c in {'CertNum','Owner'}]]\n",
      "    stones.index = pd.MultiIndex.from_tuples([(owner, cert) for owner, cert in nodupes[['Owner','CertNum']].values],\n",
      "                                             names = ['Owner', 'CertNum'])\n",
      "    return stones\n",
      "\n",
      "def cache_records(records, active, file_date):  \n",
      "    now = datetime.now()\n",
      "    atts = {\n",
      "          'file_date': file_date,\n",
      "          'records': records,\n",
      "          'active':active\n",
      "        }\n",
      "    pd.Series(atts).to_pickle(path.join(CACHE_PATH, 'rapnet.pkl'))\n",
      "    print 'cache write took {0}'.format(datetime.now() - now)\n",
      "    #s = HDFStore(path.join(CACHE_PATH, 'rapnet.h5'), complevel=9, complib='blosc')\n",
      "    #s['records'] = records\n",
      "    #s['active'] = active\n",
      "    #s['attributes'] = pd.Series({'file_date':file_date})\n",
      "    \n",
      "def load_cache():\n",
      "    now = datetime.now()\n",
      "    #h5path = path.join(CACHE_PATH, 'rapnet.h5')\n",
      "    #if path.exists(h5path):\n",
      "    #    s = H5Store(h5path)\n",
      "    #    atts = s['attributes'].to_dict()\n",
      "    #    return s['records'], s['active'], atts['file_date']\n",
      "    pklpath = path.join(CACHE_PATH, 'rapnet.pkl')\n",
      "    if path.exists(pklpath):\n",
      "        s = pd.read_pickle(pklpath).to_dict()\n",
      "        print 'cache load took {0}'.format(datetime.now() - now)\n",
      "        return s['records'], s['active'], s['file_date']\n",
      "    return [], [], []\n",
      "\n",
      "def gen_file_dates(start_day):\n",
      "    oneday = timedelta(1)\n",
      "    today = datetime.today()\n",
      "    #today = datetime.strptime('20130304', '%Y%m%d')\n",
      "    cur_day = start_day\n",
      "    while cur_day <= today:\n",
      "        yield cur_day\n",
      "        cur_day = cur_day + oneday\n",
      "        \n",
      "def get_latest(grp):\n",
      "    grp.sortlevel(level='event_day', inplace=True, ascending = False)\n",
      "    return grp.iloc[0]\n",
      "\n",
      "def build_cache():\n",
      "    all_records, prev, prev_day = load_cache()\n",
      "    first_day = datetime(2013,3,3)\n",
      "    if prev_day:\n",
      "        first_day = datetime.strptime(prev_day, '%Y%m%d') + timedelta(1)\n",
      "        print 'build_cache starting after previous load date', prev_day\n",
      "    i = 0\n",
      "    def reindex(idx_day, idx_event, df):\n",
      "        df.index = pd.MultiIndex.from_tuples([(idx_event, owner, cert, idx_day) for owner, cert in df.index],\n",
      "                                             names = ['event_type','Owner','CertNum','event_day'])\n",
      "            \n",
      "    file_date = None\n",
      "    for cur_day in gen_file_dates(first_day):\n",
      "        file_date = cur_day.strftime('%Y%m%d')\n",
      "        now = datetime.now()\n",
      "        df = read_daily_file(file_date)\n",
      "        if len(df) == 0:\n",
      "            continue\n",
      "        print 'processing file for {0}...'.format(file_date),\n",
      "        active = filter_data(df)\n",
      "        if len(prev):\n",
      "            new_stones = active.loc[active.index - prev.index]\n",
      "            kept_stones = prev.loc[active.index & prev.index]\n",
      "            readds = []\n",
      "            if len(new_stones):           \n",
      "                # cross section of data - we only want things were previously \n",
      "                # removed, to see if any of the new_stones are actually re-adds\n",
      "                all_removals = []\n",
      "                try:\n",
      "                    all_removals = all_records.xs(REMOVE, level = 'event_type')\n",
      "                except KeyError:\n",
      "                    pass\n",
      "                if len(all_removals):\n",
      "                    prev_removals = all_removals.groupby(level=['Owner','CertNum']).apply(get_latest)\n",
      "                    if len(prev_removals):\n",
      "                        readds = new_stones.loc[new_stones.index & prev_removals.index]\n",
      "                        if len(readds):\n",
      "                            newonly = new_stones.index - readds.index\n",
      "                            if len(newonly):\n",
      "                                new_stones = new_stones.loc[newonly]\n",
      "                            else:\n",
      "                                new_stones = []\n",
      "\n",
      "            if len(new_stones):\n",
      "                reindex(cur_day, ADD, new_stones)\n",
      "                all_records = pd.concat([all_records, new_stones])\n",
      "\n",
      "            if len(readds):\n",
      "                reindex(cur_day, READD, readds)\n",
      "                all_records = pd.concat([all_records, readds])\n",
      "                \n",
      "            # join the stones not removed w/ the current load to see what's changed\n",
      "            joined_px = pd.merge(kept_stones, active, left_index = True, right_index = True)\n",
      "\n",
      "            # see if any prices have changed\n",
      "            px_changes = active.loc[joined_px[(joined_px.Price_x != joined_px.Price_y) & ~(np.isnan(joined_px.Price_y))].index]\n",
      "            if len(px_changes):\n",
      "                reindex(cur_day, PRICE_CHANGE, px_changes)\n",
      "                all_records = pd.concat([all_records, px_changes])\n",
      "\n",
      "            # see what's been removed\n",
      "            removals = prev.loc[prev.index - active.index]\n",
      "            if len(removals):\n",
      "                reindex(cur_day, REMOVE, removals)\n",
      "                all_records = pd.concat([all_records, removals])\n",
      "                \n",
      "            print '{0} new stones, {1} removals, {2} price changes, {3} readds'.format(\n",
      "                len(new_stones), len(removals), len(px_changes), len(readds))\n",
      "        else:\n",
      "            # create all_records w/ multiindex: event_date, event_type, certnum\n",
      "            all_records = active.copy()\n",
      "            reindex(cur_day, ADD, all_records)\n",
      "        cache_records(all_records, active, file_date)\n",
      "        prev = active\n",
      "        gc.collect()\n",
      "        print 'took {0}'.format(datetime.now() - now)\n",
      "    return all_records, prev, file_date\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "   build_cache()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "last_entry.groupby(['Color','Clarity']).size().to_csv('/home/oliver/Dropbox/whitepine/test.csv')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}