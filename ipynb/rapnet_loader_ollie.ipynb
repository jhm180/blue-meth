{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import rapnet_loader as rl\n",
      "# uncomment and run the following line to see the source for rapnet_loader\n",
      "%load /home/oliver/ipynb/rapnet_loader.py\n",
      "\n",
      "#ADD = 1\n",
      "#REMOVE = 2\n",
      "#READD = 3\n",
      "#PRICE_CHANGE = 4\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "from os import path\n",
      "import glob\n",
      "import re\n",
      "from datetime import datetime,date,timedelta\n",
      "from pandas.io.parsers import read_csv\n",
      "import numpy as np\n",
      "from itertools import repeat\n",
      "import sys\n",
      "import traceback\n",
      "import gc\n",
      "from pandas.io.pytables import HDFStore\n",
      "import shutil\n",
      "\n",
      "ADD = 1\n",
      "REMOVE = 2\n",
      "READD = 3\n",
      "PRICE_CHANGE = 4\n",
      "\n",
      "DATA_PATH = '/home/oliver/rapnet_data'\n",
      "CACHE_PATH = '/home/oliver/rapnet_cache'\n",
      "COL_NAMES = ['LotNum', 'Owner', 'Shape', 'Carat', 'Color', 'Clarity', 'Cut Grade', 'Price', 'PctRap',\n",
      " 'Cert', 'Depth', 'Table', 'Girdle', 'Culet', 'Polish', 'Sym', 'Fluor', 'Meas', 'Comment',\n",
      " 'NumStones', 'CertNum', 'StockNum', 'Make', 'Date', 'City', 'State', 'Country', 'Image']\n",
      "\n",
      "def read_daily_file(file_date):\n",
      "    compression = None\n",
      "    daily_file = path.join(DATA_PATH, 'Rapnet_{0}_Main.csv'.format(file_date))\n",
      "    if not path.exists(daily_file):\n",
      "        daily_file = daily_file + '.gz'\n",
      "        compression = \"gzip\"\n",
      "    if not path.exists(daily_file):\n",
      "        return []\n",
      "    try:\n",
      "        return read_csv(daily_file, compression = compression, names = COL_NAMES, header = 0,\n",
      "                    #dtype = {'Owner':str, 'CertNum':str},\n",
      "                    engine = 'python')\n",
      "    except EOFError:\n",
      "        print 'bad file {0}; renaming'.format(daily_file)\n",
      "        shutil.move(daily_file, daily_file + '.bad')\n",
      "        return []\n",
      "\n",
      "# don't load fake cert nums\n",
      "bad_certs = { '123456789', '', '1234567890', '0' }\n",
      "\n",
      "def filter_data(df):\n",
      "    indices = []\n",
      "    for k, grpdf in df.groupby(['Owner','CertNum']):\n",
      "        if len(grpdf) == 1:\n",
      "            indices.append(grpdf.index[0])\n",
      "        elif not k[1] in bad_certs:\n",
      "            # we were dropping a lot of dupes, so instead i'm\n",
      "            # just adding the one w/ the highest lot num\n",
      "            indices.append(grpdf.LotNum.idxmax())\n",
      "    nodupes = df.loc[indices]\n",
      "    #dupes = df.loc[df.index - indices]\n",
      "    if len(df) - len(nodupes) > 0:\n",
      "        print 'Filtering: dropped {0} of {1} rows'.format(len(df) - len(nodupes), len(df))\n",
      "    stones = nodupes[[c for c in COL_NAMES if not c in {'CertNum','Owner'}]]\n",
      "    stones.index = pd.MultiIndex.from_tuples([(owner, cert) for owner, cert in nodupes[['Owner','CertNum']].values],\n",
      "                                             names = ['Owner', 'CertNum'])\n",
      "    return stones\n",
      "\n",
      "def cache_records(records, active, file_date):  \n",
      "    now = datetime.now()\n",
      "    atts = {\n",
      "          'file_date': file_date,\n",
      "          'records': records,\n",
      "          'active':active\n",
      "        }\n",
      "    pd.Series(atts).to_pickle(path.join(CACHE_PATH, 'rapnet.pkl'))\n",
      "    print 'cache write took {0}'.format(datetime.now() - now)\n",
      "    #s = HDFStore(path.join(CACHE_PATH, 'rapnet.h5'), complevel=9, complib='blosc')\n",
      "    #s['records'] = records\n",
      "    #s['active'] = active\n",
      "    #s['attributes'] = pd.Series({'file_date':file_date})\n",
      "    \n",
      "def load_cache():\n",
      "    now = datetime.now()\n",
      "    #h5path = path.join(CACHE_PATH, 'rapnet.h5')\n",
      "    #if path.exists(h5path):\n",
      "    #    s = H5Store(h5path)\n",
      "    #    atts = s['attributes'].to_dict()\n",
      "    #    return s['records'], s['active'], atts['file_date']\n",
      "    pklpath = path.join(CACHE_PATH, 'rapnet.pkl')\n",
      "    if path.exists(pklpath):\n",
      "        s = pd.read_pickle(pklpath).to_dict()\n",
      "        print 'cache load took {0}'.format(datetime.now() - now)\n",
      "        return s['records'], s['active'], s['file_date']\n",
      "    return [], [], []\n",
      "\n",
      "def gen_file_dates(start_day):\n",
      "    oneday = timedelta(1)\n",
      "    today = datetime.today()\n",
      "    #today = datetime.strptime('20130304', '%Y%m%d')\n",
      "    cur_day = start_day\n",
      "    while cur_day <= today:\n",
      "        yield cur_day\n",
      "        cur_day = cur_day + oneday\n",
      "        \n",
      "def get_latest(grp):\n",
      "    grp.sortlevel(level='event_day', inplace=True, ascending = False)\n",
      "    return grp.iloc[0]\n",
      "\n",
      "def build_cache():\n",
      "    all_records, prev, prev_day = load_cache()\n",
      "    first_day = datetime(2013,3,3)\n",
      "    if prev_day:\n",
      "        first_day = datetime.strptime(prev_day, '%Y%m%d') + timedelta(1)\n",
      "        print 'build_cache starting after previous load date', prev_day\n",
      "    i = 0\n",
      "    def reindex(idx_day, idx_event, df):\n",
      "        df.index = pd.MultiIndex.from_tuples([(idx_event, owner, cert, idx_day) for owner, cert in df.index],\n",
      "                                             names = ['event_type','Owner','CertNum','event_day'])\n",
      "            \n",
      "    file_date = None\n",
      "    for cur_day in gen_file_dates(first_day):\n",
      "        file_date = cur_day.strftime('%Y%m%d')\n",
      "        now = datetime.now()\n",
      "        df = read_daily_file(file_date)\n",
      "        if len(df) == 0:\n",
      "            continue\n",
      "        print 'processing file for {0}...'.format(file_date),\n",
      "        active = filter_data(df)\n",
      "        if len(prev):\n",
      "            new_stones = active.loc[active.index - prev.index]\n",
      "            kept_stones = prev.loc[active.index & prev.index]\n",
      "            readds = []\n",
      "            if len(new_stones):           \n",
      "                # cross section of data - we only want things were previously \n",
      "                # removed, to see if any of the new_stones are actually re-adds\n",
      "                all_removals = []\n",
      "                try:\n",
      "                    all_removals = all_records.xs(REMOVE, level = 'event_type')\n",
      "                except KeyError:\n",
      "                    pass\n",
      "                if len(all_removals):\n",
      "                    prev_removals = all_removals.groupby(level=['Owner','CertNum']).apply(get_latest)\n",
      "                    if len(prev_removals):\n",
      "                        readds = new_stones.loc[new_stones.index & prev_removals.index]\n",
      "                        if len(readds):\n",
      "                            newonly = new_stones.index - readds.index\n",
      "                            if len(newonly):\n",
      "                                new_stones = new_stones.loc[newonly]\n",
      "                            else:\n",
      "                                new_stones = []\n",
      "\n",
      "            if len(new_stones):\n",
      "                reindex(cur_day, ADD, new_stones)\n",
      "                all_records = pd.concat([all_records, new_stones])\n",
      "\n",
      "            if len(readds):\n",
      "                reindex(cur_day, READD, readds)\n",
      "                all_records = pd.concat([all_records, readds])\n",
      "                \n",
      "            # join the stones not removed w/ the current load to see what's changed\n",
      "            joined_px = pd.merge(kept_stones, active, left_index = True, right_index = True)\n",
      "\n",
      "            # see if any prices have changed\n",
      "            px_changes = active.loc[joined_px[joined_px.Price_x != joined_px.Price_y].index]\n",
      "            if len(px_changes):\n",
      "                reindex(cur_day, PRICE_CHANGE, px_changes)\n",
      "                all_records = pd.concat([all_records, px_changes])\n",
      "\n",
      "            # see what's been removed\n",
      "            removals = prev.loc[prev.index - active.index]\n",
      "            if len(removals):\n",
      "                reindex(cur_day, REMOVE, removals)\n",
      "                all_records = pd.concat([all_records, removals])\n",
      "                \n",
      "            print '{0} new stones, {1} removals, {2} price changes, {3} readds'.format(\n",
      "                len(new_stones), len(removals), len(px_changes), len(readds))\n",
      "        else:\n",
      "            # create all_records w/ multiindex: event_date, event_type, certnum\n",
      "            all_records = active.copy()\n",
      "            reindex(cur_day, ADD, all_records)\n",
      "        cache_records(all_records, active, file_date)\n",
      "        prev = active\n",
      "        gc.collect()\n",
      "        print 'took {0}'.format(datetime.now() - now)\n",
      "    return all_records, prev, file_date\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "   build_cache()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_df, current_df, file_date = rl.load_cache()\n",
      "# all_df is the whole database, with the event types you see in the above cell\n",
      "# the index is a multiindex with levels ('event_type','Owner','CertNum,'event_day')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "Exception",
       "evalue": "Level file_date not found",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-4-0f63ec72ea34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# removals from yesterday\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mprev_removals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mall_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPRICE_CHANGE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'20131122'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'event_type'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'file_date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/home/oliver/env/pydata/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mxs\u001b[1;34m(self, key, axis, level, copy)\u001b[0m\n\u001b[0;32m   2333\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2335\u001b[1;33m             \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_ax\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2337\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/oliver/env/pydata/lib/python2.7/site-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36mget_loc_level\u001b[1;34m(self, key, level)\u001b[0m\n\u001b[0;32m   2311\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2312\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mlev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2313\u001b[1;33m                 \u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc_level\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2314\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2315\u001b[0m                     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/oliver/env/pydata/lib/python2.7/site-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36mget_loc_level\u001b[1;34m(self, key, level)\u001b[0m\n\u001b[0;32m   2320\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_drop_levels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2322\u001b[1;33m         \u001b[0mlevel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_level_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2324\u001b[0m         \u001b[1;31m# kludge for #1796\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/oliver/env/pydata/lib/python2.7/site-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36m_get_level_number\u001b[1;34m(self, level)\u001b[0m\n\u001b[0;32m   1540\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1541\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1542\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Level %s not found'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlevel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1543\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mlevel\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1544\u001b[0m                 \u001b[0mlevel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mException\u001b[0m: Level file_date not found"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "cache load took 0:00:22.663055\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# removals from yesterday\n",
      "price_changes_1122 = all_df.xs((rl.PRICE_CHANGE, '20131122'), level=['event_type','event_day'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "float('NaN') == float('NaN')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "False"
       ]
      }
     ],
     "prompt_number": 20
    }
   ],
   "metadata": {}
  }
 ]
}